import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from docx import Document
import pandas as pd
import json
from typing import List, Dict
import concurrent.futures

load_dotenv()

st.title("üõí Maximum Recall Grocery List Generator")

# ============================================================================
# CONFIGURATION
# ============================================================================

EXTRACTION_SCHEMA = {
    "type": "json_schema",
    "json_schema": {
        "name": "ingredient_extraction",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
                "ingredients": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "quantity": {"type": "number"},
                            "unit": {"type": "string"},
                            "day": {"type": "integer"},
                            "meal": {"type": "string"},
                            "dish": {"type": "string"}
                        },
                        "required": ["name", "quantity", "unit", "day", "meal", "dish"],
                        "additionalProperties": False
                    }
                }
            },
            "required": ["ingredients"],
            "additionalProperties": False
        }
    }
}

EXTRACTION_PROMPT = """–ò–∑–≤–ª–µ–∫–∏ –í–°–ï –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –∏–∑ –º–µ–Ω—é –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ù–ï –ü–†–û–ü–£–°–¢–ò –ù–ò –û–î–ù–û–ì–û –ò–ù–ì–†–ï–î–ò–ï–ù–¢–ê:
1. –í–∫–ª—é—á–∏ –ö–ê–ñ–î–´–ô –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç, –¥–∞–∂–µ –µ—Å–ª–∏ "–ø–æ –∂–µ–ª–∞–Ω–∏—é", "–ø–æ –≤–∫—É—Å—É", "—É–∫—Ä–∞—Å–∏—Ç—å"
2. –í–∫–ª—é—á–∏ –í–°–ï —Å–ø–µ—Ü–∏–∏, –º–∞—Å–ª–∞, —Å–æ—É—Å—ã, –∑–µ–ª–µ–Ω—å - –¥–∞–∂–µ –µ—Å–ª–∏ –∫–∞–∂—É—Ç—Å—è –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏
3. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–ß–ù–´–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–∑ —Ä–µ—Ü–µ–ø—Ç–∞ (—É–∂–µ –Ω–∞ –≤—Å–µ –ø–æ—Ä—Ü–∏–∏ - –ù–ï —É–º–Ω–æ–∂–∞–π!)
4. –î–ª—è "–¥–µ—Å–µ—Ä—Ç –Ω–∞ 150 –∫–∫–∞–ª" –∏–ª–∏ "—Å–Ω–µ–∫–∏ –Ω–∞ 200 –∫–∫–∞–ª" - –≤–∫–ª—é—á–∏ –¥–æ—Å–ª–æ–≤–Ω–æ –∫–∞–∫ –µ—Å—Ç—å
5. –ù–ï –≤–∫–ª—é—á–∞–π: —Ç–æ–ª—å–∫–æ –≤–æ–¥—É –¥–ª—è –≤–∞—Ä–∫–∏ (–Ω–æ –í–ö–õ–Æ–ß–ò –ø–æ–∫—É–ø–Ω–æ–π –±—É–ª—å–æ–Ω)
6. –õ—é–±–æ–µ –º–æ–ª–æ–∫–æ ‚Üí "—Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –º–æ–ª–æ–∫–æ"

–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞:
- name: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–ø–æ–º–∏–¥–æ—Ä—ã/—Ç–æ–º–∞—Ç—ã ‚Üí "—Ç–æ–º–∞—Ç—ã", —Ç–≤—ë—Ä–¥—ã–π —Å—ã—Ä ‚Üí "—Å—ã—Ä")
- quantity: —Ç–æ—á–Ω–æ–µ —á–∏—Å–ª–æ –∏–∑ —Ä–µ—Ü–µ–ø—Ç–∞
- unit: —Ç–æ—á–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞ (–≥, –º–ª, —à—Ç, —Å—Ç–∞–∫–∞–Ω, —Å—Ç.–ª., —á.–ª.)
- day: –Ω–æ–º–µ—Ä –¥–Ω—è (1-7)
- meal: –∑–∞–≤—Ç—Ä–∞–∫/–æ–±–µ–¥/–ø–µ—Ä–µ–∫—É—Å/—É–∂–∏–Ω
- dish: –∫—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±–ª—é–¥–∞

–ü–†–û–í–ï–†–¨ –î–í–ê–ñ–î–´: –ï—Å–ª–∏ –±–ª—é–¥–æ –≥–æ—Ç–æ–≤–∏—Ç—Å—è –Ω–∞ 3 –ø–æ—Ä—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 3 –¥–Ω—è –ø–æ–¥—Ä—è–¥ - 
—É–∫–∞–∂–∏ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –û–î–ò–ù —Ä–∞–∑ –¥–ª—è –¥–Ω—è –≥–æ—Ç–æ–≤–∫–∏, –ù–ï —É–º–Ω–æ–∂–∞–π –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π!"""

# ============================================================================
# CATEGORIES
# ============================================================================

CATEGORIES = {
    "–ú–Ø–°–û, –ü–¢–ò–¶–ê, –†–´–ë–ê, –ú–û–†–ï–ü–†–û–î–£–ö–¢–´": [
        "–≥–æ–≤—è–¥", "–∫—É—Ä–∏–Ω", "—Ç—Ä–µ—Å–∫–∞", "–∫—Ä–µ–≤–µ—Ç–∫", "—Ñ–∞—Ä—à", "—Ñ–∏–ª–µ", "—Ä—ã–±", "–º–æ—Ä–µ–ø—Ä–æ–¥—É–∫—Ç"
    ],
    "–Ø–ô–¶–ê –ò –ú–û–õ–û–ß–ù–´–ï –ü–†–û–î–£–ö–¢–´": [
        "—è–π—Ü", "—Ç–≤–æ—Ä–æ–≥", "–π–æ–≥—É—Ä—Ç", "—Å—ã—Ä", "–º–æ–ª–æ–∫–æ", "—Å–ª–∏–≤–∫", "–ø–∞—Ä–º–µ–∑–∞–Ω", "–∫–µ—Ñ–∏—Ä"
    ],
    "–ö–†–£–ü–´, –ó–õ–ê–ö–ò, –ë–û–ë–û–í–´–ï, –ú–£–ö–ê": [
        "–≥—Ä–µ—á–∫", "—Ä–∏—Å", "–ø–µ—Ä–ª–æ–≤–∫", "—á–µ—á–µ–≤–∏—Ü", "–±—É–ª–≥—É—Ä", "–º—É–∫–∞", "–∫—É–∫—É—Ä—É–∑–Ω", "–ø–æ–ª–±", 
        "—Ö–ª–µ–±", "–ª–∞–≤–∞—à", "—Ö–ª–µ–±—Ü", "–æ–≤—Å—è–Ω", "–±–æ–±–æ–≤"
    ],
    "–û–í–û–©–ò, –ì–†–ò–ë–´, –ó–ï–õ–ï–ù–¨": [
        "—Ç–æ–º–∞—Ç", "–ø–æ–º–∏–¥–æ—Ä", "–æ–≥—É—Ä–µ—Ü", "–ø–µ—Ä–µ—Ü", "–ª—É–∫", "—á–µ—Å–Ω–æ–∫", "–º–æ—Ä–∫–æ–≤",
        "–±–∞–∫–ª–∞–∂–∞–Ω", "–∫–∞–±–∞—á–æ–∫", "—à–ø–∏–Ω–∞—Ç", "–≥—Ä–∏–±—ã", "—à–∞–º–ø–∏–Ω—å–æ–Ω", "–∫–∞—Ä—Ç–æ—Ñ–µ–ª—å",
        "—Ä—É–∫–∫–æ–ª–∞", "–∑–µ–ª–µ–Ω—å", "–ø–µ—Ç—Ä—É—à–∫", "—É–∫—Ä–æ–ø", "–∫–∏–Ω–∑–∞", "–±–∞–∑–∏–ª–∏–∫", "—Ç—ã–∫–≤",
        "—Å–∞–ª–∞—Ç", "–∫–∞–ø—É—Å—Ç", "—Å–≤–µ–∫–ª", "–≤–µ—à–µ–Ω–∫"
    ],
    "–§–†–£–ö–¢–´ –ò –Ø–ì–û–î–´": [
        "—è–±–ª–æ–∫", "–∫–ª—É–±–Ω–∏–∫", "—è–≥–æ–¥", "—Ñ—Ä—É–∫—Ç", "–ª–∏–º–æ–Ω", "–ª–∞–π–º", "–∞–≤–æ–∫–∞–¥–æ", "–≥—Ä—É—à", "–±–∞–Ω–∞–Ω"
    ],
    "–û–†–ï–•–ò –ò –°–ï–ú–ï–ù–ê": [
        "–æ—Ä–µ—Ö", "—Ñ—É–Ω–¥—É–∫", "–≥—Ä–µ—Ü–∫", "–º–∏–Ω–¥–∞–ª", "—Å–µ–º–µ–Ω", "–∫—É–Ω–∂—É—Ç", "—á–∏–∞", "–º–∞–∫"
    ],
    "–î–ï–°–ï–†–¢–´ –ò –°–ù–ï–ö–ò": [
        "–¥–µ—Å–µ—Ä—Ç", "—Å–Ω–µ–∫", "–±–∞—Ç–æ–Ω—á–∏–∫", "–∏–∫—Ä–∞", "–ø–∞—Å—Ç–∞ –æ—Ä–µ—Ö–æ–≤–∞—è", "–ø—Ä–æ—Ç–µ–∏–Ω", "–≤–∞—Ñ–ª"
    ],
    "–°–û–£–°–´, –ú–ê–°–õ–ê, –°–ü–ï–¶–ò–ò": [
        "–º–∞—Å–ª–æ", "—É–∫—Å—É—Å", "—Å–æ—É—Å", "—Ç–∫–µ–º–∞–ª–∏", "–≥–æ—Ä—á–∏—Ü", "–ø–∞—Å—Ç–∞ —Ç–æ–º–∞—Ç–Ω",
        "—Å–ø–µ—Ü–∏–∏", "—Å–æ–ª—å", "–ø–µ—Ä–µ—Ü", "–ø–∞–ø—Ä–∏–∫", "–∫–æ—Ä–∏–∞–Ω–¥—Ä", "—Ö–º–µ–ª–∏",
        "–∫—É—Ä–∫—É–º–∞", "—Ç–∏–º—å—è–Ω", "–æ—Ä–µ–≥–∞–Ω–æ", "–∫–æ—Ä–∏—Ü–∞", "—Ä–∞–∑—Ä—ã—Ö–ª–∏—Ç–µ–ª—å", "—Å–∞—Ö–∞—Ä",
        "–≥—Ö–∏", "–æ–ª–∏–≤–∫–æ–≤"
    ]
}

def classify_category(name: str) -> str:
    name_lower = name.lower()
    for category, keywords in CATEGORIES.items():
        if any(kw in name_lower for kw in keywords):
            return category
    return "–ü–†–û–ß–ò–ï –ò–ù–ì–†–ï–î–ò–ï–ù–¢–´"

# ============================================================================
# EXTRACTION FUNCTIONS
# ============================================================================

def read_word_file(uploaded_file):
    doc = Document(uploaded_file)
    parts = []
    for para in doc.paragraphs:
        if para.text.strip():
            parts.append(para.text)
    for table in doc.tables:
        for row in table.rows:
            row_data = [cell.text.strip() for cell in row.cells]
            parts.append(" | ".join(row_data))
    return "\n".join(parts)


def extract_with_gpt4o(client: OpenAI, menu_text: str, attempt: int = 1) -> List[Dict]:
    """Extract with GPT-4o"""
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0.3 if attempt == 1 else 0.7,  # Vary temperature
        messages=[
            {"role": "system", "content": EXTRACTION_PROMPT},
            {"role": "user", "content": menu_text}
        ],
        response_format=EXTRACTION_SCHEMA
    )
    result = json.loads(response.choices[0].message.content)
    return result["ingredients"]


def extract_with_o1(client: OpenAI, menu_text: str) -> List[Dict]:
    """Extract with o1-mini for careful reasoning"""
    response = client.chat.completions.create(
        model="o1-mini",
        messages=[
            {"role": "user", "content": f"{EXTRACTION_PROMPT}\n\n–ú–ï–ù–Æ:\n{menu_text}"}
        ]
    )
    
    # o1 doesn't support structured outputs, parse JSON from text
    content = response.choices[0].message.content
    # Find JSON in response
    start = content.find('[')
    end = content.rfind(']') + 1
    if start != -1 and end > start:
        ingredients_array = json.loads(content[start:end])
        return ingredients_array
    return []


def extract_by_day(client: OpenAI, menu_text: str, day: int) -> List[Dict]:
    """Extract ingredients for a specific day - reduces context"""
    lines = menu_text.split('\n')
    
    # Find day section
    day_start = -1
    day_end = len(lines)
    
    for i, line in enumerate(lines):
        if f'–î–µ–Ω—å {day}' in line or f'Day {day}' in line:
            day_start = i
        elif day_start != -1 and (f'–î–µ–Ω—å {day+1}' in line or f'Day {day+1}' in line):
            day_end = i
            break
    
    if day_start == -1:
        return []
    
    day_menu = '\n'.join(lines[day_start:day_end])
    
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0.1,
        messages=[
            {"role": "system", "content": EXTRACTION_PROMPT + f"\n\n–≠–ö–°–¢–†–ê–ì–ò–†–£–ô –¢–û–õ–¨–ö–û –î–õ–Ø –î–ù–Ø {day}!"},
            {"role": "user", "content": day_menu}
        ],
        response_format=EXTRACTION_SCHEMA
    )
    result = json.loads(response.choices[0].message.content)
    return result["ingredients"]


# ============================================================================
# VERIFICATION FUNCTIONS
# ============================================================================

def verify_completeness(client: OpenAI, menu_text: str, extracted_names: List[str]) -> List[str]:
    """Ask model to list any missing ingredients"""
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": f"""–ú–µ–Ω—é:\n{menu_text[:4000]}

–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã: {', '.join(extracted_names)}

–í–µ—Ä–Ω–∏ JSON –¢–û–õ–¨–ö–û —Å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –º–µ–Ω—é –Ω–æ –û–¢–°–£–¢–°–¢–í–£–Æ–¢ –≤ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö.
–ù–ï –≤–∫–ª—é—á–∞–π: –≤–æ–¥—É –¥–ª—è –≤–∞—Ä–∫–∏, –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –±–ª—é–¥–∞.
–í–ö–õ–Æ–ß–ò: –≤—Å–µ —Å–ø–µ—Ü–∏–∏, –º–∞—Å–ª–∞, —Å–æ—É—Å—ã, –∑–µ–ª–µ–Ω—å.

–§–æ—Ä–º–∞—Ç: {{"missing": ["ingredient1", "ingredient2"]}}"""},
            {"role": "user", "content": "–ö–∞–∫–∏–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –ø—Ä–æ–ø—É—â–µ–Ω—ã?"}
        ],
        response_format={"type": "json_object"}
    )
    result = json.loads(response.choices[0].message.content)
    return result.get("missing", [])


def cross_check_with_claude(menu_text: str, extracted_names: List[str]) -> List[str]:
    """Cross-check with Claude for different perspective"""
    try:
        import anthropic
        claude = anthropic.Anthropic()
        
        response = claude.messages.create(
            model="claude-sonnet-4",
            max_tokens=2000,
            temperature=0,
            messages=[{
                "role": "user",
                "content": f"""–ú–µ–Ω—é:\n{menu_text[:5000]}

–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã: {', '.join(extracted_names)}

–í–µ—Ä–Ω–∏ JSON —Å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º–∏ –∏–∑ –º–µ–Ω—é, –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï–¢ –≤ –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–º —Å–ø–∏—Å–∫–µ.
–§–æ—Ä–º–∞—Ç: {{"missing": [...]}}"""
            }]
        )
        
        result = json.loads(response.content[0].text)
        return result.get("missing", [])
    except Exception as e:
        st.warning(f"Claude –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return []


def verify_quantities(client: OpenAI, menu_text: str, df: pd.DataFrame) -> List[Dict]:
    """Spot-check quantities for critical items"""
    critical = df[df['category'].str.contains('–ú–Ø–°–û|–†–´–ë–ê|–ö–†–£–ü–´|–ú–û–õ–û–ß–ù')].head(15)
    
    items_text = "\n".join([
        f"{row['name']}: {row['quantity']}{row['unit']}" 
        for _, row in critical.iterrows()
    ])
    
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0,
        messages=[
            {"role": "system", "content": f"""–ú–µ–Ω—é:\n{menu_text[:4000]}

–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞:\n{items_text}

–í–µ—Ä–Ω–∏ JSON —Å –æ—à–∏–±–∫–∞–º–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ >30%).
–§–æ—Ä–º–∞—Ç: {{"errors": [{{"name": "x", "extracted": "100–≥", "should_be": "~300–≥"}}]}}"""},
            {"role": "user", "content": "–ï—Å—Ç—å –ª–∏ –≥—Ä—É–±—ã–µ –æ—à–∏–±–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞?"}
        ],
        response_format={"type": "json_object"}
    )
    result = json.loads(response.choices[0].message.content)
    return result.get("errors", [])


# ============================================================================
# AGGREGATION
# ============================================================================

def aggregate_ingredients(ingredients: List[Dict]) -> pd.DataFrame:
    df = pd.DataFrame(ingredients)
    
    # Clean data types to ensure they're hashable for drop_duplicates
    def clean_field(field):
        if pd.isna(field) or field is None:
            return ""
        if isinstance(field, (list, dict)):
            return str(field)
        return str(field)
    
    # Clean all fields used in duplicate detection
    df['name'] = df['name'].apply(clean_field)
    df['day'] = df['day'].apply(clean_field)
    df['meal'] = df['meal'].apply(clean_field)
    df['dish'] = df['dish'].apply(clean_field)
    
    # Remove duplicates within same extraction
    df = df.drop_duplicates(subset=['name', 'day', 'meal', 'dish'])
    
    # Clean and convert quantity to numeric
    def clean_quantity(qty):
        if pd.isna(qty) or qty is None:
            return 0
        if isinstance(qty, (int, float)):
            return float(qty)
        if isinstance(qty, str):
            # Try to extract number from string
            import re
            numbers = re.findall(r'\d+\.?\d*', str(qty))
            if numbers:
                return float(numbers[0])
        return 0
    
    df['quantity'] = df['quantity'].apply(clean_quantity)
    
    # Filter out rows with zero quantity
    df = df[df['quantity'] > 0]
    
    if len(df) == 0:
        return pd.DataFrame(columns=['name', 'quantity', 'unit', 'day', 'meal', 'dish', 'category'])
    
    # Convert day back to numeric for proper sorting
    def parse_day(day_str):
        try:
            return int(day_str) if day_str.isdigit() else 0
        except:
            return 0
    
    df['day_numeric'] = df['day'].apply(parse_day)
    
    aggregated = df.groupby(['name', 'unit'], as_index=False).agg({
        'quantity': 'sum',
        'day_numeric': lambda x: sorted(set(x)),
        'meal': lambda x: list(set(x)),
        'dish': lambda x: list(set(x))
    })
    
    # Rename back to 'day' for consistency
    aggregated = aggregated.rename(columns={'day_numeric': 'day'})
    
    aggregated['category'] = aggregated['name'].apply(classify_category)
    return aggregated


def add_piece_counts(df: pd.DataFrame) -> pd.DataFrame:
    PRODUCE_WEIGHTS = {
        "—Ç–æ–º–∞—Ç—ã": 100, "–ø–æ–º–∏–¥–æ—Ä": 100, "–æ–≥—É—Ä–µ—Ü": 150, "–ø–µ—Ä–µ—Ü": 150,
        "–ª—É–∫": 100, "–º–æ—Ä–∫–æ–≤—å": 100, "–∫–∞—Ä—Ç–æ—Ñ–µ–ª—å": 150, "—è–±–ª–æ–∫": 180,
        "–∞–≤–æ–∫–∞–¥–æ": 200, "–ª–∏–º–æ–Ω": 100, "–±–∞–∫–ª–∞–∂–∞–Ω": 300, "–∫–∞–±–∞—á–æ–∫": 250,
        "—è–π—Ü": 50
    }
    
    def calc_pieces(row):
        if row['unit'] in ['–≥', '–≥—Ä', '—à—Ç']:
            for produce, weight in PRODUCE_WEIGHTS.items():
                if produce in row['name'].lower():
                    if row['unit'] == '—à—Ç':
                        return f"{int(row['quantity'])} —à—Ç"
                    pieces = int(row['quantity'] / weight + 0.9)
                    return f"{row['quantity']}{row['unit']} (~{pieces} —à—Ç)"
        return f"{row['quantity']}{row['unit']}"
    
    df['quantity_display'] = df.apply(calc_pieces, axis=1)
    return df


def format_grocery_list(df: pd.DataFrame) -> str:
    output = []
    categories_order = [
        "–ú–Ø–°–û, –ü–¢–ò–¶–ê, –†–´–ë–ê, –ú–û–†–ï–ü–†–û–î–£–ö–¢–´",
        "–Ø–ô–¶–ê –ò –ú–û–õ–û–ß–ù–´–ï –ü–†–û–î–£–ö–¢–´",
        "–ö–†–£–ü–´, –ó–õ–ê–ö–ò, –ë–û–ë–û–í–´–ï, –ú–£–ö–ê",
        "–û–í–û–©–ò, –ì–†–ò–ë–´, –ó–ï–õ–ï–ù–¨",
        "–§–†–£–ö–¢–´ –ò –Ø–ì–û–î–´",
        "–û–†–ï–•–ò –ò –°–ï–ú–ï–ù–ê",
        "–î–ï–°–ï–†–¢–´ –ò –°–ù–ï–ö–ò",
        "–°–û–£–°–´, –ú–ê–°–õ–ê, –°–ü–ï–¶–ò–ò",
        "–ü–†–û–ß–ò–ï –ò–ù–ì–†–ï–î–ò–ï–ù–¢–´"
    ]
    
    for i, category in enumerate(categories_order, 1):
        items = df[df['category'] == category]
        if len(items) == 0:
            continue
        output.append(f"\n**{i}. {category}**\n")
        for _, row in items.iterrows():
            days_str = ", ".join([f"–î{d}" for d in row['day']])
            meals_str = ", ".join(set(row['meal']))
            dishes_str = "; ".join(list(set(row['dish']))[:3])
            output.append(
                f"- {row['name'].capitalize()} {row['quantity_display']} "
                f"({days_str} {meals_str}: {dishes_str})"
            )
    return "\n".join(output)


# ============================================================================
# MAIN APP
# ============================================================================

doc_file = st.file_uploader("üìÅ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–µ–Ω—é (.docx)", type=['docx'])

if doc_file:
    menu_text = read_word_file(doc_file)
    st.success("‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω!")
    
    with st.expander("üìÑ –ü—Ä–æ—Å–º–æ—Ç—Ä –º–µ–Ω—é"):
        st.text(menu_text[:2000] + "..." if len(menu_text) > 2000 else menu_text)
    
    if st.button("üöÄ –°–û–ó–î–ê–¢–¨ –°–ü–ò–°–û–ö (–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)"):
        client = OpenAI()
        all_ingredients = []
        
        # ===== PHASE 1: MULTI-MODEL ENSEMBLE EXTRACTION =====
        st.markdown("## üîç –§–∞–∑–∞ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ (–∞–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π)")
        
        with st.spinner("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ GPT-4o (3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–ø—ã—Ç–∫–∏ —Å —Ä–∞–∑–Ω–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π)..."):
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                futures = [
                    executor.submit(extract_with_gpt4o, client, menu_text, i)
                    for i in range(1, 4)
                ]
                gpt4o_results = [f.result() for f in futures]
            
            for result in gpt4o_results:
                all_ingredients.extend(result)
            st.success(f"‚úÖ GPT-4o: {sum(len(r) for r in gpt4o_results)} –∑–∞–ø–∏—Å–µ–π")
        
        with st.spinner("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ o1-mini (–≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑)..."):
            try:
                o1_result = extract_with_o1(client, menu_text)
                all_ingredients.extend(o1_result)
                st.success(f"‚úÖ o1-mini: {len(o1_result)} –∑–∞–ø–∏—Å–µ–π")
            except Exception as e:
                st.warning(f"o1-mini –ø—Ä–æ–ø—É—â–µ–Ω: {e}")
        
        with st.spinner("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ –¥–Ω—è–º (7 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤)..."):
            with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
                futures = [
                    executor.submit(extract_by_day, client, menu_text, day)
                    for day in range(1, 8)
                ]
                day_results = [f.result() for f in futures]
            
            for result in day_results:
                all_ingredients.extend(result)
            st.success(f"‚úÖ –ü–æ –¥–Ω—è–º: {sum(len(r) for r in day_results)} –∑–∞–ø–∏—Å–µ–π")
        
        st.info(f"üìä –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(all_ingredients)}")
        
        # ===== PHASE 2: AGGREGATION =====
        st.markdown("## üìä –§–∞–∑–∞ 2: –ê–≥—Ä–µ–≥–∞—Ü–∏—è")
        
        with st.spinner("–ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤..."):
            df = aggregate_ingredients(all_ingredients)
            df = add_piece_counts(df)
            st.success(f"‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤: {len(df)}")
        
        grocery_list = format_grocery_list(df)
        extracted_names = df['name'].unique().tolist()
        
        # ===== PHASE 3: MULTI-LAYER VERIFICATION =====
        st.markdown("## ‚úÖ –§–∞–∑–∞ 3: –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è")
        
        all_missing = []
        
        # Layer 1: Full menu check
        with st.spinner("–°–ª–æ–π 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã (GPT-4o)..."):
            missing_1 = verify_completeness(client, menu_text, extracted_names)
            all_missing.extend(missing_1)
            st.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö: {len(missing_1)}")
        
        # Layer 2: Day-by-day verification
        with st.spinner("–°–ª–æ–π 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –¥–Ω—è–º (7 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö)..."):
            with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:
                futures = []
                for day in range(1, 8):
                    day_ingredients = df[df['day'].apply(lambda x: day in x)]['name'].tolist()
                    futures.append(
                        executor.submit(verify_completeness, client, menu_text, day_ingredients)
                    )
                day_missing = [item for f in futures for item in f.result()]
            
            all_missing.extend(day_missing)
            st.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ –¥–Ω—è–º: {len(day_missing)}")
        
        # Layer 3: Cross-model check with Claude
        with st.spinner("–°–ª–æ–π 3: –ö—Ä–æ—Å—Å-–ø—Ä–æ–≤–µ—Ä–∫–∞ (Claude)..."):
            claude_missing = cross_check_with_claude(menu_text, extracted_names)
            all_missing.extend(claude_missing)
            st.info(f"Claude –Ω–∞—à—ë–ª: {len(claude_missing)}")
        
        # Layer 4: Quantity verification
        with st.spinner("–°–ª–æ–π 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤..."):
            qty_errors = verify_quantities(client, menu_text, df)
            if qty_errors:
                st.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –æ—à–∏–±–æ–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞: {len(qty_errors)}")
                st.json(qty_errors)
        
        # Deduplicate and clean missing list
        all_missing = list(set([m.lower().strip() for m in all_missing if m]))
        
        # ===== PHASE 4: AUTO-FIX =====
        if all_missing:
            st.warning(f"‚ö†Ô∏è –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {len(all_missing)} –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤")
            st.write(all_missing)
            
            with st.spinner("üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö..."):
                # Re-extract just the missing ones
                missing_prompt = f"""–ò–∑ —ç—Ç–æ–≥–æ –º–µ–Ω—é –∏–∑–≤–ª–µ–∫–∏ –¢–û–õ–¨–ö–û —Å–ª–µ–¥—É—é—â–∏–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã: {', '.join(all_missing)}

–ú–µ–Ω—é:\n{menu_text}

{EXTRACTION_PROMPT}"""
                
                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": missing_prompt},
                        {"role": "user", "content": "–ò–∑–≤–ª–µ–∫–∏ —ç—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã"}
                    ],
                    response_format=EXTRACTION_SCHEMA
                )
                
                fixed = json.loads(response.choices[0].message.content)
                all_ingredients.extend(fixed['ingredients'])
                
                # Re-aggregate
                df = aggregate_ingredients(all_ingredients)
                df = add_piece_counts(df)
                grocery_list = format_grocery_list(df)
                
                st.success(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(fixed['ingredients'])} –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤. –ò—Ç–æ–≥–æ: {len(df)}")
        else:
            st.success("üéâ –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã - –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ!")
        
        # ===== FINAL OUTPUT =====
        st.markdown("---")
        st.markdown("## üõí –§–ò–ù–ê–õ–¨–ù–´–ô –°–ü–ò–°–û–ö –ü–û–ö–£–ü–û–ö")
        st.markdown(grocery_list)
        
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å —Å–ø–∏—Å–æ–∫",
            data=grocery_list,
            file_name="shopping_list.txt",
            mime="text/plain"
        )
        
        # Stats
        st.markdown("### üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        col1, col2, col3 = st.columns(3)
        col1.metric("–í—Å–µ–≥–æ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤", len(df))
        col2.metric("–ó–∞–ø–∏—Å–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–æ", len(all_ingredients))
        col3.metric("–î–æ–±–∞–≤–ª–µ–Ω–æ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ", len(all_missing))